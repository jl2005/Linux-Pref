# 案例汇总

## docker启动慢

### 环境准备

```
docker run --name tomcat --cpus 0.1 -m 512M -p 8080:8080 -itd feisky/tomcat:8
```

### 现场状况

```
for ((i=0;i<30;i++)); do curl localhost:8080; sleep 1; done
```

PS: 本地docker环境没有复现不断重启的现象

### 定位问题

#### 查看docker的状态

```
docker inspect tomcat -f '{{json .State}}' | jq
```

#### 查看Java 堆内存

```
查看堆内存,注意单位是字节
docker exec tomcat java -XX:+PrintFlagsFinal -version | grep HeapSize
```

#### 定位启动慢的原因

```
# docker rm -f tomcat 
# 运行新容器
$ docker run --name tomcat --cpus 0.1 -m 512M -e JAVA_OPTS='-Xmx512m -Xms512m' -p 8080:8 
# 执行 pidstat 
$ pidstat -t -p `docker inspect tomcat -f "{{.State.Pid}}"` 1
```

CPU使用率低，`%wait`占用较高，最后确定是由于CPU做了限制，所以%wait 较高。

## 容器丢包

### 环境搭建

```
docker run --name nginx --hostname nginx --privileged -p 80:80 -itd feisky/nginx:drop
```

###  问题复现

```
# hping3 -c 10 -S -p 80 172.17.0.2
HPING 172.17.0.2 (docker0 172.17.0.2): S set, 40 headers + 0 data bytes
len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=2 win=65535 rtt=1011.4 ms
len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=4 win=65535 rtt=0.9 ms
DUP! len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=2 win=65535 rtt=3060.4 ms
len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=5 win=65535 rtt=1019.6 ms
DUP! len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=4 win=65535 rtt=3108.1 ms
len=44 ip=172.17.0.2 ttl=64 DF id=0 sport=80 flags=SA seq=8 win=65535 rtt=1026.3 ms

--- 172.17.0.2 hping statistic ---
10 packets tramitted, 6 packets received, 40% packet loss
round-trip min/avg/max = 0.9/1537.8/3108.1 ms
```

### 链路层问题排查

#### 查看网络丢包

```
# netstat -i
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0       100      118      0      6 0            48      0      0      0 BMRU
lo       65536        0      0      0 0             0      0      0      0 LRU
```

#### 查看网络策略

```
# tc -s qdisc show dev eth0
qdisc netem 8001: root refcnt 2 limit 1000 loss 30%
 Sent 2542 bytes 47 pkt (dropped 14, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
```

其中，`qdisc netem 8001: root refcnt 2 limit 1000 loss 30%`表示:

eth0 上面配置了一个网络模拟排队规则(qdisc netem),并且配置了丢包率为 30%(loss 30%)。再看后面的统计信息,发送了 47 个包,但是丢了 14 个。

#### 删除限流规则

```
tc qdisc del dev eth0 root netem loss 30%
```

### 网络层和传输层


```
# netstat -s
Ip:
    Forwarding: 1                    // 开启转发
    124 total packets received
    0 forwarded
    0 incoming packets discarded     // 接收丢包数
    82 incoming packets delivered    // 接收数据包数
    70 requests sent out
    1 outgoing packets dropped
Icmp:
    0 ICMP messages received
    0 input ICMP message failed
    ICMP input histogram:
    0 ICMP messages sent
    0 ICMP messages failed
    ICMP output histogram:
Tcp:
    0 active connection openings     // 主动连接
    1 passive connection openings    // 被动连接
    32 failed connection attempts    // 失败连接尝试数
    0 connection resets received
    0 connections established
    82 segments received
    102 segments sent out
    38 segments retransmitted        // 重传报文
    0 bad segments received
    1 resets sent
Udp:
    0 packets received
    0 packets to unknown port received
    0 packet receive errors
    0 packets sent
    0 receive buffer errors
    0 send buffer errors
UdpLite:
TcpExt:
    32 resets received for embryonic SYN_RECV sockets
    0 packet headers predicted
    1 acknowledgments not containing data payload received
    TCPTimeouts: 53
    1 connections reset due to unexpected data
    TCPSynRetrans: 37
    TCPOrigDataSent: 1
    TCPDelivered: 1
    TcpTimeoutRehash: 1
IpExt:
    InOctets: 5004
    OutOctets: 3112
    InNoECTPkts: 124
```

### iptables

#### 查看连接跟综数

```
# 主机终端中查询内核配置
# sysctl net.netfilter.nf_conntrack_max
net.netfilter.nf_conntrack_max = 65536
# sysctl net.netfilter.nf_conntrack_count
net.netfilter.nf_conntrack_count = 1
```

这个是查看宿主机的连接跟综数，是否需要查看docker中的连接跟综数

#### 定位丢包

```
# iptables -t filter -nvL
Chain INPUT (policy ACCEPT 82 packets, 3324 bytes)
 pkts bytes target     prot opt in     out     source               destination
   42  1680 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 70 packets, 3112 bytes)
 pkts bytes target     prot opt in     out     source               destination
   33  1456 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981
```

#### 删除丢包策略

```
# iptables -t filter -D INPUT -m statistic --mode random --probability 0.30 -j DROP
# iptables -t filter -D OUTPUT -m statistic --mode random --probability 0.30 -j DROP
```

### tcpdump


#### 抓包定位丢失

```
tcpdump -i eth0 -nn port 80
```

发现建联包都可以收到，但是http请求包会丢失

#### 查看丢包统计

```
# netstat -i
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0      1500      222      0     80 0           126      0      0      0 BMRU
lo       65536        0      0      0 0             0      0      0      0 LRU
```

#### MTU 问题修复

通过ifconfig发现mtu为100，然后使用如下的方式进行修改

```
ifconfig eth0 mtu 1500
```

## 















----
